{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import contextlib\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, OPTConfig\n",
    "from models.Qformer import *\n",
    "from utils.data import build_dataloader_from_yaml\n",
    "import torch.nn.functional as F\n",
    "cur_device = torch.device(\"cuda:6\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_query_token=32,\n",
    "            clip_vec_len=1408,\n",
    "            sam_vec_len=4096,\n",
    "            cls_num = 14\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.clip_qformer = Qformer(fecture_vec_len=clip_vec_len, num_query_token=num_query_token+1, cross_attention_freq=2)\n",
    "        self.sam_qformer = Qformer(fecture_vec_len=sam_vec_len, num_query_token=num_query_token+1, cross_attention_freq=2)\n",
    "        self.cls_num = cls_num\n",
    "        \n",
    "        self.fc1 = nn.Linear(1536, 100)  # 输入大小为1536，输出大小为100\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 42)    # 输入大小为100，输出大小为10\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.lossfn = nn.BCELoss()\n",
    "        self.target = torch.rand((4,42,))\n",
    "\n",
    "    def forward(self, samples):\n",
    "        clip_features = samples['clip_features']\n",
    "        sam_features = samples['sam_features']\n",
    "        # text_input = samples['text_input']\n",
    "        cur_device = clip_features.device\n",
    "        # print(\"clip shape: \", clip_features.shape, \"sam shape: \", sam_features.shape)\n",
    "\n",
    "        # CLIP\n",
    "        clip_query_output = self.clip_qformer(\n",
    "            features=clip_features,\n",
    "            attention_mask=torch.ones(clip_features.size()[:-1], dtype=torch.long).to(cur_device)\n",
    "        )\n",
    "\n",
    "        # SAM\n",
    "        sam_features = sam_features.to(cur_device)\n",
    "        sam_query_output = self.sam_qformer(\n",
    "            features=sam_features,\n",
    "            attention_mask=torch.ones(sam_features.size()[:-1], dtype=torch.long).to(cur_device),\n",
    "        )\n",
    "\n",
    "        clip_cls = sam_query_output[:, -1, :]\n",
    "        sam_cls = clip_query_output[:, -1, :]\n",
    "\n",
    "        cat_cls = torch.cat([clip_cls, sam_cls], dim=1)\n",
    "        print(cat_cls.shape)\n",
    "        x = self.fc1(cat_cls)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, 14, 3)\n",
    "        x = F.softmax(x, dim=2)\n",
    "        x = x.view(-1, 42)\n",
    "        print(x)\n",
    "        loss = self.lossfn(x, self.target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simplenet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(1536, 100)  # 输入大小为1536，输出大小为100\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 42)    # 输入大小为100，输出大小为10\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "        self.target = torch.rand((42*4,))\n",
    "    \n",
    "    def forward(self, cat_cls):\n",
    "        # text_input = samples['text_input']\n",
    "        # print(\"clip shape: \", clip_features.shape, \"sam shape: \", sam_features.shape)\n",
    "\n",
    "        # CLIP\n",
    "\n",
    "        \n",
    "        # MLP\n",
    "        x = self.fc1(cat_cls)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, 14, 3)\n",
    "        x = F.softmax(x, dim=2)\n",
    "        x = x.view(-1, 42)\n",
    "        print(x)\n",
    "        # loss = self.lossfn(x, self.target)\n",
    "        # return loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anet = simplenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(mlp_model, samples, simplenet):\n",
    "    clip_features = samples['clip_features']\n",
    "    sam_features = samples['sam_features']\n",
    "    # text_input = samples['text_input']\n",
    "    cur_device = clip_features.device\n",
    "    # print(\"clip shape: \", clip_features.shape, \"sam shape: \", sam_features.shape)\n",
    "\n",
    "    # CLIP\n",
    "    clip_query_output = mlp_model.clip_qformer(\n",
    "        features=clip_features,\n",
    "        attention_mask=torch.ones(clip_features.size()[:-1], dtype=torch.long).to(cur_device)\n",
    "    )\n",
    "\n",
    "    # SAM\n",
    "    sam_features = sam_features.to(cur_device)\n",
    "    sam_query_output = mlp_model.sam_qformer(\n",
    "        features=sam_features,\n",
    "        attention_mask=torch.ones(sam_features.size()[:-1], dtype=torch.long).to(cur_device),\n",
    "    )\n",
    "    print(sam_query_output.shape)\n",
    "    clip_cls = sam_query_output[:, -1, :]\n",
    "    sam_cls = clip_query_output[:, -1, :]\n",
    "    print(clip_cls.shape, sam_cls.shape)\n",
    "    cat_cls = torch.cat([clip_cls, sam_cls], dim=1)\n",
    "    print(cat_cls.shape)\n",
    "    simplenet(cat_cls)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "configpath = \"/home/xcg/medical-research/Project23us/config/train.yaml\"\n",
    "custom_dataloader = build_dataloader_from_yaml(configpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xcg/medical-research/Project23us/test_mlp.ipynb 单元格 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     print(clip_feature.shape, sam_feature.shape)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     my_samples \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39msam_features\u001b[39m\u001b[39m'\u001b[39m: sam_feature\u001b[39m.\u001b[39mview(sam_shape[\u001b[39m0\u001b[39m], sam_shape[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39msam_shape[\u001b[39m2\u001b[39m], sam_shape[\u001b[39m3\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mclip_features\u001b[39m\u001b[39m'\u001b[39m: clip_feature\u001b[39m.\u001b[39mview(clip_shape[\u001b[39m0\u001b[39m], clip_shape[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mclip_shape[\u001b[39m2\u001b[39m], clip_shape[\u001b[39m3\u001b[39m]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mtext_input\u001b[39m\u001b[39m'\u001b[39m: caption,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     }\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     loss \u001b[39m=\u001b[39m mlp_model(my_samples)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.105.100.230/home/xcg/medical-research/Project23us/test_mlp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp_model' is not defined"
     ]
    }
   ],
   "source": [
    "for clip_feature, sam_feature, caption in custom_dataloader:\n",
    "#     clip_feature = clip_feature.to(cur_device)\n",
    "#     sam_feature = sam_feature.to(cur_device)\n",
    "    clip_shape = clip_feature.shape\n",
    "    sam_shape = sam_feature.shape\n",
    "#     print(clip_feature.shape, sam_feature.shape)\n",
    "    my_samples = {\n",
    "            'sam_features': sam_feature.view(sam_shape[0], sam_shape[1]*sam_shape[2], sam_shape[3]),\n",
    "            'clip_features': clip_feature.view(clip_shape[0], clip_shape[1]*clip_shape[2], clip_shape[3]),\n",
    "            'text_input': caption,\n",
    "    }\n",
    "    \n",
    "    loss = mlp_model(my_samples)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7561, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.runner import *\n",
    "from utils.data import build_dataloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MLPRunner(RunnerBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        cfg,\n",
    "    ):\n",
    "        config = self.build_config(cfg)\n",
    "        optimizer = self.build_optimizer(model, config)\n",
    "        dataloader = build_dataloader(config)\n",
    "        max_epoch = config[\"run\"][\"max_epoch\"]\n",
    "        device = config[\"run\"][\"device\"]\n",
    "        device = torch.device('cpu')\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            dataloader=dataloader,\n",
    "            max_epoch=max_epoch,\n",
    "            device=device,\n",
    "        )\n",
    "        self.config = config\n",
    "    \n",
    "    def train_step(self, samples):\n",
    "        clip_shape = samples[0].shape\n",
    "        sam_shape = samples[1].shape\n",
    "        \n",
    "        # print(samples[1].view(clip_shape[0], clip_shape[1]*clip_shape[2], clip_shape[3]).shape)\n",
    "        my_samples = {\n",
    "            'sam_features': samples[1].view(sam_shape[0], sam_shape[1]*sam_shape[2], sam_shape[3]).to(self.device),\n",
    "            'clip_features': samples[0].view(clip_shape[0], clip_shape[1]*clip_shape[2], clip_shape[3]).to(self.device),\n",
    "            'text_input': samples[2],\n",
    "        }\n",
    "        # print(my_samples)\n",
    "        loss = self.model(my_samples)\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(self):\n",
    "        for samples in tqdm(self.dataloader):\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                loss = self.train_step(samples)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "    def epoch_start_hook(self, info):\n",
    "        pass\n",
    "\n",
    "    def epoch_end_hook(self, info):\n",
    "        # also save MLP\n",
    "        torch.save({\n",
    "            'epoch': info['cur_epoch'],  # 假设你训练了5个epochs\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, f\"./checkpoints/mlp_checkpoint_{info['cur_epoch']}.pth\")\n",
    "        print(info)\n",
    "\n",
    "    def build_config(self, cfg):\n",
    "        with open(cfg, 'r') as file:\n",
    "            _config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        return _config\n",
    "    \n",
    "    @classmethod\n",
    "    def build_optimizer(self, model, config):\n",
    "        # lr_scale = config[\"run\"][\"lr_layer_decay\"]\n",
    "        # weight_decay = config[\"run\"][\"weight_decay\"]\n",
    "        optim_params = model.parameters()\n",
    "        # optim_params = self.model.Parameters()\n",
    "\n",
    "        # num_parameters = 0\n",
    "        # for p_group in optim_params:\n",
    "        #     for p in p_group[\"params\"]:\n",
    "        #         num_parameters += p.data.nelement()    \n",
    "        # logging.info(\"number of trainable parameters: {}\".format(num_parameters))      \n",
    "                \n",
    "        beta2 = config[\"run\"][\"beta2\"]\n",
    "\n",
    "        _optimizer = torch.optim.AdamW(\n",
    "            optim_params,\n",
    "            lr=float(config[\"run\"][\"init_lr\"]),\n",
    "            betas=(0.9, beta2),\n",
    "        )    \n",
    "        return _optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcg/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "runner = MLPRunner(mlp_model, \"/home/xcg/medical-research/Project23us/config/train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train_epoch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
