{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import yaml\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# dataset_base = \"/data2/xcg_data/lavis_data/2023us/features\"\n",
    "# csvpath = \"/data/xcg/lavis_data/coco-2023us/excels/translated.csv\"\n",
    "jsonpath = \"/home/xcg/medical-research/Project23us/labels/8000patient.json\"\n",
    "dataset_base = \"/data2/xcg_data/lavis_data/2023us/features\"\n",
    "csvpath = \"/data/xcg/lavis_data/coco-2023us/excels/translated.csv\"\n",
    "# Define your custom dataset class\n",
    "class Dataset_2023us(Dataset):\n",
    "    def __init__(self):\n",
    "        self.jsonpath = jsonpath\n",
    "        self.dataset_base = dataset_base\n",
    "        self.limitation = 8\n",
    "        with open(jsonpath , 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        # self.keylist = list(self.data.keys())\n",
    "        self.searchset = self.load_searchset()\n",
    "        self.pairs, self.keylist = self.load_pairs()\n",
    "    \n",
    "    def load_searchset(self):\n",
    "        searchset = {}\n",
    "        imageids = os.listdir( self.dataset_base + \"/clip_features\")\n",
    "        for imageid in imageids:\n",
    "            personid = imageid.split(\"_\")[1]\n",
    "            if personid not in searchset.keys():\n",
    "                searchset[personid] = [imageid]\n",
    "            else:\n",
    "                searchset[personid].append(imageid)\n",
    "        return searchset\n",
    "    \n",
    "    def load_pairs(self):\n",
    "        # {personid: [[image_ids, ], [probabilities...  3*11 ] ], }\n",
    "        pairs = {}\n",
    "        keylist = []\n",
    "        for personid in self.data.keys():\n",
    "            # if personid in self.searchset.keys():\n",
    "            #     imglist = self.searchset[personid]\n",
    "            #     probabilitylist = []\n",
    "            #     for organ in self.data[personid].keys():\n",
    "            #         for mark in self.data[personid][organ].keys():\n",
    "            #             probabilitylist+=self.data[personid][organ][mark]                    \n",
    "            \n",
    "            #     pairs[personid] = [imglist, probabilitylist]\n",
    "            #     keylist.append(personid)\n",
    "            try:\n",
    "            # print(len(self.data[personid].keys()))\n",
    "                imglist = self.searchset[personid]\n",
    "                probabilitylist = []\n",
    "                for organ in self.data[personid].keys():\n",
    "                    \n",
    "                    probabilitylist+=self.data[personid][organ][\"good\"]\n",
    "                pairs[personid] = [imglist, probabilitylist]\n",
    "                keylist.append(personid)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "        return pairs, keylist\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keylist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # clip_feature, sam_feature, caption\n",
    "        personid = self.keylist[index]\n",
    "        clip_feature = None\n",
    "        sam_feature = None\n",
    "        pairlen = len(self.pairs[personid][0])\n",
    "        if pairlen <= self.limitation:\n",
    "            for i in range(self.limitation):\n",
    "                clip_feature_path = self.dataset_base + \"/clip_features/\" + self.pairs[personid][0][i % pairlen] \n",
    "                sam_feature_path = self.dataset_base + \"/sam_features/\" + self.pairs[personid][0][i % pairlen] \n",
    "                clip_dataloads = np.load(clip_feature_path)\n",
    "                sam_dataloads = np.load(sam_feature_path)\n",
    "                if clip_feature is None:\n",
    "                    clip_feature = torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                    sam_feature = torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                else:\n",
    "                    clip_feature = torch.cat([clip_feature, torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "                    sam_feature = torch.cat([sam_feature, torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "            \n",
    "        else:\n",
    "            # print(\"pairlen: \", pairlen)\n",
    "            cls_list = []\n",
    "            imgid_list = []\n",
    "            for i in range(pairlen):\n",
    "                clip_feature_path = self.dataset_base + \"/clip_features/\" + self.pairs[personid][0][i] \n",
    "                clip_dataloads = np.load(clip_feature_path)\n",
    "                clip_cls = torch.from_numpy(clip_dataloads[\"arr\"])[0]\n",
    "                cls_list.append(clip_cls)\n",
    "                imgid_list.append(self.pairs[personid][0][i])\n",
    "            \n",
    "            vectors = torch.stack(cls_list, dim=0)\n",
    "            \n",
    "            normalized_vectors = F.normalize(vectors, p=2, dim=1)\n",
    "\n",
    "            normalized_vectors = normalized_vectors.numpy()\n",
    "\n",
    "            kmeans = KMeans(n_clusters=self.limitation, random_state=0)\n",
    "\n",
    "            cluster_labels = kmeans.fit_predict(normalized_vectors)\n",
    "\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "            \n",
    "            cosine_sims = cosine_similarity(normalized_vectors, cluster_centers)\n",
    "\n",
    "            for i in range(self.limitation):\n",
    "                cluster_indices = np.where(cluster_labels == i)[0]\n",
    "                cluster_similarities = cosine_sims[cluster_indices, i]\n",
    "                representative_index = cluster_indices[np.argmax(cluster_similarities)]\n",
    "                selected_imgid = imgid_list[representative_index]\n",
    "                # print(\"index: \", representative_index)\n",
    "                \n",
    "                clip_feature_path = self.dataset_base + \"/clip_features/\" + selected_imgid \n",
    "                sam_feature_path = self.dataset_base + \"/sam_features/\" + selected_imgid \n",
    "                clip_dataloads = np.load(clip_feature_path)\n",
    "                sam_dataloads = np.load(sam_feature_path)\n",
    "                if clip_feature is None:\n",
    "                    clip_feature = torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                    sam_feature = torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                else:\n",
    "                    clip_feature = torch.cat([clip_feature, torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "                    sam_feature = torch.cat([sam_feature, torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "         \n",
    "        return clip_feature, sam_feature, torch.tensor(self.pairs[personid][1])\n",
    "\n",
    "def build_mlp_dataloader():\n",
    "# (batchsize, limitation, 677, 1408) (batchsize, limitation, 256, 4096) 6*caption\n",
    "    batch_size = 4\n",
    "    shuffle = True\n",
    "    datas = Dataset_2023us()\n",
    "    custom_dataloader = DataLoader(datas, batch_size=batch_size, shuffle=shuffle)\n",
    "    return custom_dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myset = Dataset_2023us()\n",
    "# print(len(myset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.runner import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MLPRunner(RunnerBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        cfg,\n",
    "    ):\n",
    "        config = self.build_config(cfg)\n",
    "        optimizer = self.build_optimizer(model, config)\n",
    "        dataloader = build_mlp_dataloader()\n",
    "        max_epoch = config[\"run\"][\"max_epoch\"]\n",
    "        device = config[\"run\"][\"device\"]\n",
    "        # device = torch.device('cpu')\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            dataloader=dataloader,\n",
    "            max_epoch=max_epoch,\n",
    "            device=device,\n",
    "        )\n",
    "        self.config = config\n",
    "    \n",
    "    def train_step(self, samples):\n",
    "        clip_shape = samples[0].shape\n",
    "        sam_shape = samples[1].shape\n",
    "        \n",
    "        # print(samples[1].view(clip_shape[0], clip_shape[1]*clip_shape[2], clip_shape[3]).shape)\n",
    "        my_samples = {\n",
    "            'sam_features': samples[1].view(sam_shape[0], sam_shape[1]*sam_shape[2], sam_shape[3]).to(self.device),\n",
    "            'clip_features': samples[0].view(clip_shape[0], clip_shape[1]*clip_shape[2], clip_shape[3]).to(self.device),\n",
    "            'target': samples[2].to(self.device),\n",
    "        }\n",
    "        # print(my_samples)\n",
    "        # model = model.to(self.device)\n",
    "        loss = self.model(my_samples)\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(self):\n",
    "        for samples in tqdm(self.dataloader):\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                loss = self.train_step(samples)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "    def epoch_start_hook(self, info):\n",
    "        pass\n",
    "\n",
    "    def epoch_end_hook(self, info):\n",
    "        # also save MLP\n",
    "        # \n",
    "        # samblip: Qformer, 32*query token, project(linear)\n",
    "        # mlpcls: cls token, mlp\n",
    "        torch.save({\n",
    "            'epoch': info['cur_epoch'],  # 假设你训练了5个epochs\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, f\"./checkpoints/mlp_checkpoint_{info['cur_epoch']}.pth\")\n",
    "        print(info)\n",
    "\n",
    "    def build_config(self, cfg):\n",
    "        with open(cfg, 'r') as file:\n",
    "            _config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        return _config\n",
    "    \n",
    "    @classmethod\n",
    "    def build_optimizer(self, model, config):\n",
    "        # lr_scale = config[\"run\"][\"lr_layer_decay\"]\n",
    "        # weight_decay = config[\"run\"][\"weight_decay\"]\n",
    "        optim_params = model.parameters()\n",
    "        # optim_params = self.model.Parameters()\n",
    "\n",
    "        # num_parameters = 0\n",
    "        # for p_group in optim_params:\n",
    "        #     for p in p_group[\"params\"]:\n",
    "        #         num_parameters += p.data.nelement()    \n",
    "        # logging.info(\"number of trainable parameters: {}\".format(num_parameters))      \n",
    "                \n",
    "        beta2 = config[\"run\"][\"beta2\"]\n",
    "\n",
    "        _optimizer = torch.optim.AdamW(\n",
    "            optim_params,\n",
    "            lr=float(config[\"run\"][\"init_lr\"]),\n",
    "            betas=(0.9, beta2),\n",
    "        )    \n",
    "        return _optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from models.mlp import ClassficationMLP\n",
    "model = ClassficationMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/xcg/medical-research/Project23us/checkpoints/mlp_untrained_0.pth\", map_location = \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = \"/home/xcg/medical-research/Project23us/config/train.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runner = MLPRunner(model, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7620)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# F.binary_cross_entropy(a1, a2)\n",
    "lossfn = torch.nn.BCEWithLogitsLoss()\n",
    "lossfn(a1, a2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "39d5901a824bba7fd1c9b9b437fbaae776370ac4d50da897d976b2fdd3a309c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
