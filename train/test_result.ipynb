{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcg/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import yaml\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# dataset_base = \"/data2/xcg_data/lavis_data/2023us/features\"\n",
    "# csvpath = \"/data/xcg/lavis_data/coco-2023us/excels/translated.csv\"\n",
    "# jsonpath = \"/home/xcg/medical-research/Project23us/labels/8000patient.json\"\n",
    "# dataset_base = \"/data2/xcg_data/lavis_data/2023us/features\"\n",
    "\n",
    "dataset_base = \"/data2/xcg_data/lavis_data/Breast_images/features\"\n",
    "jsonpath = \"/home/xcg/medical-research/Project23us/labels/breast_train.json\"\n",
    "set_base = \"/data2/xcg_data/lavis_data/Breast_images/train\"\n",
    "\n",
    "# csvpath = \"/data/xcg/lavis_data/coco-2023us/excels/translated.csv\"\n",
    "# Define your custom dataset class\n",
    "class Dataset_2023us(Dataset):\n",
    "    def __init__(self):\n",
    "        self.jsonpath = jsonpath\n",
    "        self.dataset_base = dataset_base\n",
    "        self.limitation = 8\n",
    "        with open(jsonpath , 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        # self.keylist = list(self.data.keys())\n",
    "        self.searchset = self.load_searchset()\n",
    "        self.pairs, self.keylist = self.load_pairs()\n",
    "    \n",
    "    def load_searchset(self):\n",
    "        searchset = {}\n",
    "        imageids = os.listdir(set_base)\n",
    "        for imageid in imageids:\n",
    "            personid = imageid.split(\"_\")[0] # 注意这里1改成了0\n",
    "            if personid not in searchset.keys():\n",
    "                searchset[personid] = [imageid + '.npz']\n",
    "            else:\n",
    "                searchset[personid].append(imageid + '.npz')\n",
    "        return searchset\n",
    "    \n",
    "    def load_pairs(self):\n",
    "        # {personid: [[image_ids, ], [probabilities...  3*11 ] ], }\n",
    "        pairs = {}\n",
    "        keylist = []\n",
    "        for personid in self.data.keys():\n",
    "            if personid not in self.searchset:\n",
    "                continue\n",
    "            # if personid in self.searchset.keys():\n",
    "            #     imglist = self.searchset[personid]\n",
    "            #     probabilitylist = []\n",
    "            #     for organ in self.data[personid].keys():\n",
    "            #         for mark in self.data[personid][organ].keys():\n",
    "            #             probabilitylist+=self.data[personid][organ][mark]                    \n",
    "            \n",
    "            #     pairs[personid] = [imglist, probabilitylist]\n",
    "            #     keylist.append(personid)\n",
    "            try:\n",
    "            # print(len(self.data[personid].keys()))\n",
    "                imglist = self.searchset[personid]\n",
    "                probabilitylist = []\n",
    "                for organ in self.data[personid].keys():\n",
    "                    \n",
    "                    probabilitylist+=self.data[personid][organ][\"good\"]\n",
    "                pairs[personid] = [imglist, probabilitylist]\n",
    "                keylist.append(personid)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "        return pairs, keylist\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keylist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # clip_feature, sam_feature, caption\n",
    "        personid = self.keylist[index]\n",
    "        clip_feature = None\n",
    "        sam_feature = None\n",
    "        pairlen = len(self.pairs[personid][0])\n",
    "        if pairlen <= self.limitation:\n",
    "            for i in range(self.limitation):\n",
    "                clip_feature_path = self.dataset_base + \"/clip_features/\" + self.pairs[personid][0][i % pairlen] \n",
    "                sam_feature_path = self.dataset_base + \"/sam_features/\" + self.pairs[personid][0][i % pairlen] \n",
    "                clip_dataloads = np.load(clip_feature_path)\n",
    "                sam_dataloads = np.load(sam_feature_path)\n",
    "                if clip_feature is None:\n",
    "                    clip_feature = torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                    sam_feature = torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                else:\n",
    "                    clip_feature = torch.cat([clip_feature, torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "                    sam_feature = torch.cat([sam_feature, torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "            \n",
    "        else:\n",
    "            # print(\"pairlen: \", pairlen)\n",
    "            cls_list = []\n",
    "            imgid_list = []\n",
    "            for i in range(pairlen):\n",
    "                clip_feature_path = self.dataset_base + \"/clip_features/\" + self.pairs[personid][0][i] \n",
    "                clip_dataloads = np.load(clip_feature_path)\n",
    "                clip_cls = torch.from_numpy(clip_dataloads[\"arr\"])[0]\n",
    "                cls_list.append(clip_cls)\n",
    "                imgid_list.append(self.pairs[personid][0][i])\n",
    "            \n",
    "            vectors = torch.stack(cls_list, dim=0)\n",
    "            \n",
    "            normalized_vectors = F.normalize(vectors, p=2, dim=1)\n",
    "\n",
    "            normalized_vectors = normalized_vectors.numpy()\n",
    "\n",
    "            kmeans = KMeans(n_clusters=self.limitation, random_state=0)\n",
    "\n",
    "            cluster_labels = kmeans.fit_predict(normalized_vectors)\n",
    "\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "            \n",
    "            cosine_sims = cosine_similarity(normalized_vectors, cluster_centers)\n",
    "\n",
    "            for i in range(self.limitation):\n",
    "                cluster_indices = np.where(cluster_labels == i)[0]\n",
    "                cluster_similarities = cosine_sims[cluster_indices, i]\n",
    "                representative_index = cluster_indices[np.argmax(cluster_similarities)]\n",
    "                selected_imgid = imgid_list[representative_index]\n",
    "                # print(\"index: \", representative_index)\n",
    "                \n",
    "                clip_feature_path = self.dataset_base + \"/clip_features/\" + selected_imgid \n",
    "                sam_feature_path = self.dataset_base + \"/sam_features/\" + selected_imgid \n",
    "                clip_dataloads = np.load(clip_feature_path)\n",
    "                sam_dataloads = np.load(sam_feature_path)\n",
    "                if clip_feature is None:\n",
    "                    clip_feature = torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                    sam_feature = torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)\n",
    "                else:\n",
    "                    clip_feature = torch.cat([clip_feature, torch.from_numpy(clip_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "                    sam_feature = torch.cat([sam_feature, torch.from_numpy(sam_dataloads[\"arr\"]).unsqueeze(0)], dim=0)\n",
    "         \n",
    "        return clip_feature, sam_feature, torch.tensor(self.pairs[personid][1])\n",
    "\n",
    "def build_mlp_dataloader():\n",
    "# (batchsize, limitation, 677, 1408) (batchsize, limitation, 256, 4096) 6*caption\n",
    "    batch_size = 4\n",
    "    shuffle = True\n",
    "    datas = Dataset_2023us()\n",
    "    custom_dataloader = DataLoader(datas, batch_size=batch_size, shuffle=shuffle)\n",
    "    return custom_dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = \"/home/xcg/medical-research/Project23us/checkpoints/mlp_checkpoint_1.pth\"\n",
    "checkpoint = \"/data2/xcg_data/lavis_data/Breast_images/checkpoint/breast_checkpoint_3.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset_2023us()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.keylist[0]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.mlp import ClassficationMLP\n",
    "model = ClassficationMLP()\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jsonpath, 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(checkpoint, map_location \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    218\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint, map_location = \"cpu\")['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0.446094 0.030602 0.523304\n",
      "res:  [1.0, 0.0, 0.0]\n",
      "pred: 0.443148 0.013429 0.543423\n",
      "res:  [1.0, 0.0, 0.0]\n",
      "pred: 0.446018 0.033275 0.520708\n",
      "res:  [1.0, 0.0, 0.0]\n",
      "pred: 0.445752 0.036259 0.517990\n",
      "res:  [1.0, 0.0, 0.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m testcnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(data)):\n\u001b[0;32m----> 9\u001b[0m     samples \u001b[39m=\u001b[39m data[i]\n\u001b[1;32m     10\u001b[0m     personid \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mkeylist[i]\n\u001b[1;32m     11\u001b[0m     clip_shape \u001b[39m=\u001b[39m samples[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m, in \u001b[0;36mDataset_2023us.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m             clip_feature \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([clip_feature, torch\u001b[39m.\u001b[39mfrom_numpy(clip_dataloads[\u001b[39m\"\u001b[39m\u001b[39marr\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m             sam_feature \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([sam_feature, torch\u001b[39m.\u001b[39mfrom_numpy(sam_dataloads[\u001b[39m\"\u001b[39;49m\u001b[39marr\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[39m# print(\"pairlen: \", pairlen)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     cls_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(\u001b[39mbytes\u001b[39;49m,\n\u001b[1;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_pickle,\n\u001b[1;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_header_size)\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/format.py:824\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    822\u001b[0m             read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[1;32m    823\u001b[0m             read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[0;32m--> 824\u001b[0m             data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39;49m\u001b[39marray data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    825\u001b[0m             array[i:i\u001b[39m+\u001b[39mread_count] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    826\u001b[0m                                                      count\u001b[39m=\u001b[39mread_count)\n\u001b[1;32m    828\u001b[0m \u001b[39mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/format.py:959\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     \u001b[39m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     \u001b[39m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 959\u001b[0m         r \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(size \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(data))\n\u001b[1;32m    960\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[1;32m    961\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(r) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m size:\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/zipfile.py:940\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    939\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[0;32m--> 940\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[1;32m    941\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m    942\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/anaconda3/envs/medical-tmp/lib/python3.8/zipfile.py:1016\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m   1015\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m-> 1016\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[1;32m   1017\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1020\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "count = 0\n",
    "count_correct = 0\n",
    "wrong_ill = 0\n",
    "test_notill09 = 0\n",
    "test_notill08 = 0\n",
    "testcnt = 0\n",
    "for i in range(0, len(data)):\n",
    "    samples = data[i]\n",
    "    personid = data.keylist[i]\n",
    "    clip_shape = samples[0].unsqueeze(0).shape\n",
    "    sam_shape = samples[1].unsqueeze(0).shape\n",
    "#     print(clip_shape, sam_shape)\n",
    "    # res = []\n",
    "    # for organ in label[personid].keys():\n",
    "    #     res+=(label[personid][organ]['good'])\n",
    "    res = label[personid]['breast']['good']\n",
    "    # res = np.array(res)\n",
    "    my_samples = {\n",
    "            'sam_features': samples[1].unsqueeze(0).view(sam_shape[0], sam_shape[1]*sam_shape[2], sam_shape[3]).to(device),\n",
    "            'clip_features': samples[0].unsqueeze(0).view(clip_shape[0], clip_shape[1]*clip_shape[2], clip_shape[3]).to(device),\n",
    "            'target': samples[2].to(device),\n",
    "    }\n",
    "    pred = np.array(model.predict_cls(my_samples)[0].cpu())\n",
    "    count += 1\n",
    "    pred_res = 0 # not ill\n",
    "    if pred[15] > 0.9:\n",
    "        test_notill09 += 1\n",
    "    if pred[15] > 0.8:\n",
    "        test_notill08 += 1\n",
    "    if pred[17] > pred[15]:\n",
    "        pred_res = 1 # ill\n",
    "    if res[2] == pred_res:\n",
    "        count_correct += 1\n",
    "    elif res[2] == 1:\n",
    "        wrong_ill += 1 # 事实有病，判定成没病\n",
    "        testcnt += 1\n",
    "        if testcnt >= 5:\n",
    "            continue\n",
    "        print(\"pred: %.6f %.6f %.6f\" % (pred[15], pred[16], pred[17]))\n",
    "        print(\"res: \", res)\n",
    "    elif res[0] == 1:\n",
    "        testcnt += 1\n",
    "        if testcnt >= 5:\n",
    "            continue\n",
    "        print(\"pred: %.6f %.6f %.6f\" % (pred[15], pred[16], pred[17]))\n",
    "        print(\"res: \", res)\n",
    "    # if 1 - res[0] < 0.1:\n",
    "    # # if 1 - res[2] < 0.1:\n",
    "    #     print(\"pred: %.6f %.6f %.6f\" % (pred[15], pred[16], pred[17]))\n",
    "    #     print(\"res: \", res)\n",
    "    #     flag += 1\n",
    "    # if flag == 10:\n",
    "    #     break\n",
    "print(count_correct, count, count_correct * 1.0 / count, wrong_ill)\n",
    "print(test_notill09)\n",
    "print(test_notill08)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7384)\n"
     ]
    }
   ],
   "source": [
    "lossfn = torch.nn.BCEWithLogitsLoss()\n",
    "predx = torch.Tensor([[0.455741, 0.130912, 0.413347], [0.455741, 0.130912, 0.413347]])\n",
    "resx = torch.Tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])\n",
    "print(lossfn(predx, resx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 262\n"
     ]
    }
   ],
   "source": [
    "# sample test\n",
    "with open(jsonpath , 'r') as f0:\n",
    "    dict0 = json.load(f0)\n",
    "cnt = 0 \n",
    "for item in dict0.keys():\n",
    "    if dict0[item]['breast']['good'][0] == 1:\n",
    "        cnt += 1\n",
    "print(cnt, len(dict0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000002 0.999998 0.000000\n",
      "0.000026 0.999974 0.000000\n",
      "0.000001 1.000000 0.000000\n",
      "0.000004 0.999992 0.000004\n",
      "0.000002 0.999998 0.000000\n",
      "0.999759 0.000029 0.000212\n",
      "0.000000 1.000000 0.000000\n",
      "0.000006 0.999994 0.000000\n",
      "0.000001 0.999999 0.000000\n",
      "0.000077 0.999921 0.000002\n",
      "0.000000 1.000000 0.000000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(pred), 3):\n",
    "    print(\"%.6f %.6f %.6f\" % (pred[i], pred[i+1], pred[i+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 0.000000 1.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n",
      "0.000000 1.000000 0.000000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(res), 3):\n",
    "    print(\"%.6f %.6f %.6f\" % (res[i], res[i+1], res[i+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.06969550e-09, 1.00000000e+00, 4.30273417e-09, 1.85890077e-08,\n",
       "       1.00000000e+00, 4.09027985e-11, 1.13474226e-08, 1.00000000e+00,\n",
       "       6.78733303e-15, 2.01559054e-08, 1.00000000e+00, 1.44939865e-08,\n",
       "       2.24072185e-13, 1.00000000e+00, 6.33612141e-13, 4.59914442e-13,\n",
       "       1.00000000e+00, 3.21097153e-13, 1.47211941e-12, 1.00000000e+00,\n",
       "       1.20648400e-14, 7.07796985e-08, 9.99999881e-01, 5.24248236e-08,\n",
       "       7.19498328e-09, 9.99999881e-01, 1.36981996e-07, 2.13877671e-09,\n",
       "       1.00000000e+00, 4.27888613e-09, 1.06366560e-09, 1.00000000e+00,\n",
       "       2.73354495e-09], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.66666105e-03, 9.94035482e-01, 3.29781999e-03, 1.13516820e-04,\n",
       "       3.54137563e-04, 9.99532342e-01, 5.28671034e-03, 9.88151193e-01,\n",
       "       6.56211330e-03, 2.23736255e-03, 9.91846144e-01, 5.91651723e-03,\n",
       "       3.89642105e-03, 9.51179624e-01, 4.49239574e-02, 5.30497078e-03,\n",
       "       9.79931772e-01, 1.47632211e-02, 4.11865953e-03, 9.55174923e-01,\n",
       "       4.07064557e-02, 9.36306734e-03, 9.12919700e-01, 7.77172148e-02,\n",
       "       1.41603807e-02, 7.96574652e-01, 1.89264968e-01, 1.57989649e-04,\n",
       "       4.80675226e-04, 9.99361336e-01, 1.99081679e-03, 9.94901180e-01,\n",
       "       3.10799014e-03])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.83184096e-04,  1.76310539e-03, -1.17993410e-03, -6.14695780e-04,\n",
       "        1.91891193e-03, -1.30424938e-03, -1.01127002e-03,  3.51619720e-03,\n",
       "       -2.50492198e-03, -8.28722066e-04,  3.48448753e-03, -2.65574724e-03,\n",
       "       -6.67272718e-04,  2.16680765e-03, -1.49959733e-03, -1.29007234e-03,\n",
       "        5.34713268e-03, -4.05707862e-03, -1.66833645e-03,  8.01047623e-01,\n",
       "       -7.99379289e-01, -1.66756732e-03,  1.44436955e-02, -1.27761081e-02,\n",
       "       -1.20562731e-03,  1.02667212e-02, -9.06119095e-03, -7.82661686e-04,\n",
       "        2.08491087e-03, -1.30219280e-03, -5.55502686e-04,  1.66958570e-03,\n",
       "       -1.11407698e-03])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred - res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
