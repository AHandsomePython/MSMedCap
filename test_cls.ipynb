{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.data import build_dataloader_from_yaml\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = build_dataloader_from_yaml(\"/home/xcg/medical-research/Project23us/config/train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairlen:  6\n",
      "index:  3\n",
      "index:  4\n",
      "index:  5\n",
      "index:  1\n",
      "pairlen:  5\n",
      "index:  2\n",
      "index:  1\n",
      "index:  3\n",
      "index:  0\n",
      "pairlen:  7\n",
      "index:  6\n",
      "index:  5\n",
      "index:  0\n",
      "index:  1\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for clip_feature, sam_feature, caption in loader:\n",
    "    cnt += 1\n",
    "    if cnt == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_base = \"/data2/xcg_data/lavis_data/2023us/features\"\n",
    "csv_path = \"/data/xcg/lavis_data/coco-2023us/excels/translated.csv\"\n",
    "\n",
    "class Dataset_cls(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_base, csvpath, limitation = 4):\n",
    "        self.csvpath = csvpath\n",
    "        self.dataset_base = dataset_base\n",
    "        self.pairs, self.keylist = self.load_caption()\n",
    "        self.limitation = limitation\n",
    "    \n",
    "    def load_caption(self):\n",
    "\n",
    "        # {personid: [[image_ids, ], caption], }\n",
    "        pairs = {}\n",
    "        with open(self.csvpath, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                personid = str(row[0]).split(\"_\")[1]\n",
    "                if personid not in pairs:\n",
    "                    pairs[personid] = [[row[0]], row[1]]\n",
    "                else:\n",
    "                    pairs[personid][0].append(row[0])\n",
    "\n",
    "        return pairs, list(pairs.keys())\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keylist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # clip_feature, sam_feature, caption\n",
    "        personid = self.keylist[index]\n",
    "        pairlen = len(self.pairs[personid][0])\n",
    "        cls_list = []\n",
    "        imgid_list = []\n",
    "        cls_dict = {}\n",
    "        for i in range(pairlen):\n",
    "            clip_feature_path = self.dataset_base + \"/clip_features/\" + self.pairs[personid][0][i] + '.npz'\n",
    "            clip_dataloads = np.load(clip_feature_path)\n",
    "            clip_cls = torch.from_numpy(clip_dataloads[\"arr\"])[0]\n",
    "            cls_list.append(clip_cls)\n",
    "            imgid_list.append(self.pairs[personid][0][i])\n",
    "            # cls_dict[self.pairs[personid][0][i]] = clip_cls\n",
    "\n",
    "        return cls_list, imgid_list\n",
    "        # return cls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset = Dataset_cls( dataset_base, csv_path, limitation = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1408])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myset[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.0717,  0.0560, -0.4620,  ...,  0.1498,  0.6572, -0.0380]), tensor([-0.0921,  0.0730, -0.4331,  ...,  0.0272,  0.6542,  0.0873]), tensor([ 0.0191,  0.0920, -0.4268,  ...,  0.0809,  0.6470, -0.1222]), tensor([-0.0272,  0.0735, -0.3529,  ...,  0.1641,  0.6196, -0.2456]), tensor([ 0.1014,  0.1429, -0.4235,  ...,  0.1438,  0.6310, -0.1794]), tensor([ 0.1037,  0.0642, -0.4444,  ...,  0.0939,  0.6034,  0.1693])]\n",
      "['20230212_11702789_154720983', '20230212_11702789_154745899', '20230212_11702789_154910734', '20230212_11702789_154707151', '20230212_11702789_154713016', '20230212_11702789_154750553']\n"
     ]
    }
   ],
   "source": [
    "for clip_cls, imgid in myset:\n",
    "    print(clip_cls)\n",
    "    print(imgid)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0036,  0.0097, -0.0148,  ...,  0.0228,  0.0361,  0.0104])\n",
      "tensor([ 0.0059,  0.0135, -0.0190,  ...,  0.0308,  0.0457,  0.0092])\n",
      "tensor([ 0.0031,  0.0065, -0.0253,  ...,  0.0315,  0.0400,  0.0053])\n",
      "tensor([ 0.0033,  0.0040, -0.0228,  ...,  0.0277,  0.0474,  0.0047])\n",
      "tensor([ 0.0056,  0.0023, -0.0177,  ...,  0.0285,  0.0393,  0.0032])\n",
      "tensor([ 0.0059,  0.0103, -0.0221,  ...,  0.0304,  0.0406,  0.0034])\n",
      "tensor([ 0.0059,  0.0044, -0.0338,  ...,  0.0325,  0.0456,  0.0045])\n",
      "tensor([-0.0025,  0.0059, -0.0350,  ...,  0.0356,  0.0409,  0.0063])\n",
      "tensor([ 0.0037,  0.0003, -0.0333,  ...,  0.0308,  0.0429, -0.0003])\n",
      "tensor([ 0.0145, -0.0008, -0.0216,  ...,  0.0221,  0.0250,  0.0035])\n",
      "tensor([ 0.0023,  0.0123, -0.0217,  ...,  0.0352,  0.0428,  0.0054])\n",
      "4\n",
      "8\n",
      "7\n",
      "10\n",
      "6\n",
      "3\n",
      "9\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcg/anaconda3/envs/medical/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "for clip_cls, imgid in myset:\n",
    "    if len(clip_cls) < 8:\n",
    "        continue\n",
    "    \n",
    "    vectors = torch.stack(clip_cls, dim=0)\n",
    "    \n",
    "    normalized_vectors = F.normalize(vectors, p=2, dim=1)\n",
    "    \n",
    "    for i in range(normalized_vectors.shape[0]):\n",
    "        print(normalized_vectors[i])\n",
    "\n",
    "    normalized_vectors = normalized_vectors.numpy()\n",
    "\n",
    "    kmeans = KMeans(n_clusters=8, random_state=0)\n",
    "\n",
    "    cluster_labels = kmeans.fit_predict(normalized_vectors)\n",
    "\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    cosine_sims = cosine_similarity(normalized_vectors, cluster_centers)\n",
    "\n",
    "    for i in range(8):\n",
    "        cluster_indices = np.where(cluster_labels == i)[0]\n",
    "        cluster_similarities = cosine_sims[cluster_indices, i]\n",
    "        representative_index = cluster_indices[np.argmax(cluster_similarities)]\n",
    "        # print(imgid[representative_index])\n",
    "        print(representative_index)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/xcg/medical-research/Project23us/labels/8000patient.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "alist = nn.ModuleList([nn.Linear(6, 3) for _ in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blist = []\n",
    "for i in range(4):\n",
    "    blist.append(alist[i](inputs))\n",
    "x = torch.stack(blist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
